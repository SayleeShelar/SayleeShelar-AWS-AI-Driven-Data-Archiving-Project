{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install required dependencies in the notebook\n",
    "!pip install boto3 sagemaker scikit-learn==1.5.2 joblib\n",
    "\n",
    "# Import necessary libraries\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "# Define variables\n",
    "bucket_name = \"saylee-s3-archive-bucket\"\n",
    "role = get_execution_role()  # Use your SageMaker execution role\n",
    "region = boto3.Session().region_name\n",
    "model_tarball = \"model.tar.gz\"\n",
    "\n",
    "# Upload dataset to S3\n",
    "data = pd.read_csv(\"../data/metadata.csv\")\n",
    "s3 = boto3.client(\"s3\")\n",
    "data_file = \"metadata.csv\"\n",
    "data.to_csv(data_file, index=False)\n",
    "s3.upload_file(data_file, bucket_name, data_file)\n",
    "print(f\"Uploaded dataset to s3://{bucket_name}/{data_file}\")\n",
    "\n",
    "# Prepare model tarball\n",
    "with tarfile.open(model_tarball, \"w:gz\") as tar:\n",
    "    tar.add(\"../model/inference.py\")\n",
    "    tar.add(\"../model/model.joblib\")\n",
    "\n",
    "s3.upload_file(model_tarball, bucket_name, model_tarball)\n",
    "print(f\"Uploaded model tarball to s3://{bucket_name}/{model_tarball}\")\n",
    "\n",
    "# Create SageMaker SKLearn Estimator\n",
    "framework_version = \"0.23-1\"\n",
    "script_path = \"../model/inference.py\"\n",
    "\n",
    "sklearn_model = SKLearn(\n",
    "    entry_point=script_path,\n",
    "    role=role,\n",
    "    framework_version=framework_version,\n",
    "    instance_type=\"ml.m5.large\",  # Choose an appropriate instance type\n",
    "    model_uri=f\"s3://{bucket_name}/{model_tarball}\",\n",
    ")\n",
    "\n",
    "# Deploy the model\n",
    "predictor = sklearn_model.deploy(initial_instance_count=1, instance_type=\"ml.t2.medium\")\n",
    "print(\"Model deployed successfully.\")\n",
    "\n",
    "# Testing the endpoint\n",
    "import json\n",
    "\n",
    "# Prepare a test payload (replace with appropriate data structure)\n",
    "test_payload = json.dumps({\"data\": [45, 2048]})  # Example: 45 days, 2048 bytes\n",
    "\n",
    "# Use the predictor for inference\n",
    "response = predictor.predict(test_payload)\n",
    "print(\"Prediction result:\", response)\n",
    "\n",
    "# Clean up (optional: uncomment to delete resources)\n",
    "# predictor.delete_endpoint()\n",
    "# print(\"Endpoint deleted.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
